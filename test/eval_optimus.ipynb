{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heegyukim\\Desktop\\project\\latent-diffusion-lm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.checkpoint_management has been moved to tensorflow.python.checkpoint.checkpoint_management. The old module will be deleted in version 2.9.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.resource has been moved to tensorflow.python.trackable.resource. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.util has been moved to tensorflow.python.checkpoint.checkpoint. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base_delegate has been moved to tensorflow.python.trackable.base_delegate. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.graph_view has been moved to tensorflow.python.checkpoint.graph_view. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.python_state has been moved to tensorflow.python.trackable.python_state. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.functional_saver has been moved to tensorflow.python.checkpoint.functional_saver. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.checkpoint_options has been moved to tensorflow.python.checkpoint.checkpoint_options. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "from src.task.optimus_v2 import OptimusTask as OptimusTaskV2\n",
    "from src.task.optimus import OptimusTask as OptimusTaskV1\n",
    "from coop.metric import levenshtein_batch, levenshtein\n",
    "import torch\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of OptimusDecoder were not initialized from the model checkpoint at skt/kogpt2-base-v2 and are newly initialized: ['transformer.linear_mem.weight', 'transformer.linear_emb.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ckpt = \"checkpoint/optimus-v2-44M/optimus-v2-tiny-vae.ckpt\"\n",
    "ckpt = \"checkpoint/optimus-v2-56M/optimus-v2-mini-vae.ckpt\"\n",
    "ckpt = \"checkpoint/optimus-v1-base-500k.ckpt\"\n",
    "task = OptimusTaskV1.load_from_checkpoint(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1c183d10730>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요\n",
      "오늘도, 즐거운 하루~ᄒ\n",
      "살다 보면, 이런 날도 있는 것 아니겠니?\n",
      "그러니 가끔은 좀 편하게 쉬어도 된다.\n",
      "살다 보면 이런 날 것 아니니?\"도 그러니 가끔은 좀 편하게 쉬어도 된다.\n",
      "우리 아들, 서울대 합격했어!\n",
      "차 산지 1년이나 됐는데 여태 몰라서 못썼네요\n",
      "얼음주 일본이 미개했었다는 수준 잘보고갑니다.\n",
      "틸러슨 장관은 16일 워싱턴에서 미국 외교부장과 만나 북미, 한반도에 관해 논의할 예정이다.\n",
      "당초 가족들은 유행을 추진하였으나 대통령과 전직 대통령 예장에 대한 우적인 국민열전으로 치러졌다.\n",
      "컴퓨터 과론이 활발했지만, 그 보다 컴퓨터로서의 진보이 있어서 경영혁명에 과학실이 완전히 가치 있는 윤리로서 과학적 합리주의에 관하여 할 수 있다.\n",
      "이 상태에서 물귀도 흡혈귀로 모으지만, 전염 시에는 메이즈 퀘스트를 모두 캐스팅 시간과 일정량의 시 소모한다.\n",
      "이 대화에서 김재규는 자신이 하는 것은 한 사람이라면 김계원준에게 확고한 믿음을 나타내기 위해 준비를 했던 것으로 전해지고 말이다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'이 대화에서 김재규는 자신이 하는 것은 한 사람이라면 김계원준에게 확고한 믿음을 나타내기 위해 준비를 했던 것으로 전해지고 말이다.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reconstruct(text, show=True):\n",
    "    latent = task.encode(text)\n",
    "    rec = task.generate(latent, max_length=64, num_beams=4)[0]\n",
    "    if show:\n",
    "        print(rec)\n",
    "    return rec\n",
    "\n",
    "reconstruct(\"안녕하세요\")\n",
    "reconstruct(\"오늘도, 즐거운 하루~ ㅎㅎ\")\n",
    "reconstruct(\"살다 보면, 이런 날도 있는 것 아니겠니?\")\n",
    "reconstruct(\"그러니 가끔은 좀 편하게 쉬어도 된다.\")\n",
    "reconstruct(\"살다 보면, 이런 날도 있는 것 아니겠니? 그러니 가끔은 좀 편하게 쉬어도 된다.\")\n",
    "reconstruct(\"우리 아들, 서울대 합격했어!\")\n",
    "reconstruct(\"차 산지 1년이나 됐는데 여태 몰라서 못썼네요\")\n",
    "reconstruct(\"얼음맥주 일본이 미개했었다는 수준 잘보고갑니다.\")\n",
    "reconstruct(\"틸러슨 장관은 16일 워싱턴에서 미국외교장관과 만나 북미, 남북대화에 관해 논의할 예정이다.\")\n",
    "reconstruct(\"당초 유족들은 가족장을 추진하였으나 전직 대통령에 대한 예우와 전 국민적인 추모열기로 국민장으로 치러졌다.\")\n",
    "reconstruct(\"컴퓨터 과학을 둘러싼 논의가 활발해졌지만, 그 전체로서 컴퓨터 과학이 경영관리의 혁신에 있어서 진실로 가치 있는 과학체계임을 명백히 할 필요가 있다.\")\n",
    "reconstruct(\"이 상태에서 누굴 물어도 흡혈귀로 전염시키지는 않지만, 메모라이즈 시와 캐스팅 시 모두 일정량의 피를 소모한다.\")\n",
    "reconstruct(\"이 대화에서 김재규가 자신은 한다면 하는 사람이라고 한 것은 김계원에게 확고한 결의를 보여주고 믿음을 주기 위해 했던 말인 것으로 짐작된다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration heegyu--vae_eval-13750f85c770fa6b\n",
      "Reusing dataset parquet (C:\\Users\\heegyukim\\.cache\\huggingface\\datasets\\parquet\\heegyu--vae_eval-13750f85c770fa6b\\0.0.0\\0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n",
      "100%|██████████| 8/8 [11:13<00:00, 84.18s/ba]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentence': \"그는 ▲인적자본 개발의 기회 불평등 ▲중상류층의 불공정한 기회 가로채기 등을 청년 불평등 원인으로 꼽았다.김 기획총장은 '중상류층의 경우 노동시장에서 큰 가치로 인정되는 능력을 개발할 수 있는 기회가 많다.\",\n",
       " 'reconstruct': \"그는 차별점 위주의 기회중 ◇공정경제 기회를 잡으려는 고소득층 등의 원인으로 꼽았다.중반기갑상장은 중소기업에서 소득 분배의 기회를 창출하려는 다양한 분야에서 가치가 있는 기업 중재로 인해 '생산적 가치가 높다.\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reconstruct_batch(x):\n",
    "    r = task.generate(task.encode(x[\"sentence\"]), max_length=64, early_stopping=True)\n",
    "    return {\n",
    "        \"reconstruct\": r\n",
    "    }\n",
    "\n",
    "\n",
    "eval_ds = load_dataset(\"heegyu/vae_eval\", split=\"test\", use_auth_token=True)\n",
    "eval_ds = eval_ds.map(reconstruct_batch, batched=True, batch_size=64)\n",
    "eval_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31383562075115323"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levenshtein_batch(eval_ds[\"sentence\"], eval_ds[\"reconstruct\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:  ㅋㅋ 오늘도 앉아서 게임밖에 안함\n",
      "요즘 롤하는거 진짜 존잼임 9.576796531677246\n",
      "요즘 롤하는거 진짜 존잼임, 요즘은 진짜 게임밖에 안함 ㅋㅋㅋ. 하 맨날 이러고 살았으면 좋겠다 진짜 10.333292007446289\n",
      "엄마 오늘 저녁은 고기 먹고싶다. 8.797966003417969\n",
      "할 일도 없고 너무 심심한듯 8.793122291564941\n"
     ]
    }
   ],
   "source": [
    "# 유사한 문장 길이, 겹치는 단어가 많아야 하는 듯\n",
    "\n",
    "def cosine_simm(text, targets):\n",
    "    src = task.encode(text)  # (1, 512)\n",
    "    tgt = task.encode(targets) # (n, 512)\n",
    "    scores = torch.matmul(src, tgt.T)[0]\n",
    "    print(\"source: \", text)\n",
    "    for t, s in zip(targets, scores.tolist()):\n",
    "        print(t, s)\n",
    "\n",
    "def euclid_simm(text, targets):\n",
    "    src = task.encode(text)  # (1, 512)\n",
    "    tgt = task.encode(targets) # (n, 512)\n",
    "    src = src.repeat((tgt.shape[0], 1))\n",
    "    dist = torch.sqrt(((tgt - src) ** 2).sum(-1))\n",
    "\n",
    "    print(\"source: \", text)\n",
    "    for t, s in zip(targets, dist.tolist()):\n",
    "        print(t, s)\n",
    "\n",
    "text = \"ㅋㅋ 오늘도 앉아서 게임밖에 안함\"\n",
    "targets = [\n",
    "    \"요즘 롤하는거 진짜 존잼임\",\n",
    "    \"요즘 롤하는거 진짜 존잼임, 요즘은 진짜 게임밖에 안함 ㅋㅋㅋ. 하 맨날 이러고 살았으면 좋겠다 진짜\",\n",
    "    \"엄마 오늘 저녁은 고기 먹고싶다.\",\n",
    "    \"할 일도 없고 너무 심심한듯\"\n",
    "]\n",
    "euclid_simm(text, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arithmetic 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "우리 아들, 서울대 합격했어! - 서울대 + 고려대 = 우리 아들, 고려대 합격했다! )\n",
      "우리 아들, 서울대 합격했어! - 아들 + 딸 = 우리 딸, 서울대 합격했어 딸!\n",
      "오빠, 요즘 회사생활은 어때? - 회사 + 학교 = 오빠, 요즘 학교생활은 카트?\n",
      "오빠, 요즘 회사생활은 어때? - 오빠 회사 + 언니 학교 = 언니, 요즘 학교생활은 들어?\n",
      "나 요즘 고양이 키우는데 너무 귀엽더라 ㅎㅎ - 고양이 + 강아지 = 나 강아지 요즘 키우는데 너무 11 강아지 직전\n",
      "나 요즘 고양이 키우는데 너무 귀엽더라 ㅎㅎ - 귀엽다 + 사랑스럽다 = 나 그럼 동물 키우는데 너무스럽더라\n"
     ]
    }
   ],
   "source": [
    "def arithmetic(src, sub, add):\n",
    "    x = task.encode([src, sub, add])\n",
    "    z = x[0] - x[1] + x[2]\n",
    "    print(src, \"-\", sub, \"+\", add, \"=\", task.generate(z.unsqueeze(0))[0])\n",
    "\n",
    "arithmetic(\n",
    "    \"우리 아들, 서울대 합격했어!\",\n",
    "    \"서울대\",\n",
    "    \"고려대\"\n",
    ")\n",
    "\n",
    "arithmetic(\n",
    "    \"우리 아들, 서울대 합격했어!\",\n",
    "    \"아들\",\n",
    "    \"딸\"\n",
    ")\n",
    "\n",
    "arithmetic(\n",
    "    \"오빠, 요즘 회사생활은 어때?\",\n",
    "    \"회사\",\n",
    "    \"학교\"\n",
    ")\n",
    "\n",
    "arithmetic(\n",
    "    \"오빠, 요즘 회사생활은 어때?\",\n",
    "    \"오빠 회사\",\n",
    "    \"언니 학교\"\n",
    ")\n",
    "\n",
    "arithmetic(\n",
    "    \"나 요즘 고양이 키우는데 너무 귀엽더라 ㅎㅎ\",\n",
    "    \"고양이\",\n",
    "    \"강아지\"\n",
    ")\n",
    "\n",
    "arithmetic(\n",
    "    \"나 요즘 고양이 키우는데 너무 귀엽더라 ㅎㅎ\",\n",
    "    \"귀엽다\",\n",
    "    \"사랑스럽다\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 벌써 친구와 먹고 기차를 생겼다.\n",
      "이 시동차데 왜 굿?15ㅅ\n",
      "딱 성명시은 머리를 오듯 단식에 따니 많다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def summarize(texts):\n",
    "    texts = texts.strip().split(\"\\n\")\n",
    "    x = task.encode(texts).mean(0)\n",
    "    print(task.generate(x.unsqueeze(0), max_length=64, num_beams=4)[0])\n",
    "\n",
    "summarize(\"\"\"\n",
    "나는 오늘 친구와 만났다. \n",
    "나는 친구와 피자를 먹고 맥주를 마셨다. \n",
    "나와 친구는 밥을 먹고 보드게임 카페를 갔다. \n",
    "거기서 할리갈리를 했는데 매우 재미 있었다.\n",
    "\"\"\")\n",
    "\n",
    "summarize(\"\"\"\n",
    "이 겜 진짜 핵노잼인데\n",
    "이럴거면 게임 왜 만듦?? ㅋㅋㅋ ㅈㄴ 노어이\n",
    "노잼이네요, 회사 문 닫으셈 ㅅㄱㅋ\n",
    "\"\"\")\n",
    "\n",
    "summarize(\"\"\"\n",
    "잠시 마음을 놓는 순간 거세지는 게 이곳의 바람. \n",
    "마을을 떠나는 길에는 구름까지 잔뜩 몰려오더니 결국 비바람이 몰아치기 시작했다. \n",
    "하필 오늘은 펭귄 서식지에 오기로 한 날.\n",
    "풀들이 다 휘어버릴 만큼 거센 바람을 보니 여기 사는 펭귄들이 걱정됐다. \n",
    "여기에서 돌아갈 수도 없다. \n",
    "이런 날씨에 펭귄이 있을까 걱정했는데 입구부터 귀여운 울음소리가 들닌다. \n",
    "손님을 처음으로 반겨준 이 녀석. \n",
    "장난감 나팔 같은 목소리가 독특하다.\n",
    "얼음이 아닌 따뜻한 땅을 좋아하는 마젤란 펭귄은 이곳에 집을 짓고 매년 봄, 여름을 보낸다. \n",
    "신기하게도 같은 집에서 같은 암수가 매년 만나 이렇게 예쁜 새끼를 낳고 여름 내내 수영을 가르쳐서 가을이 되면 더 따뜻한 브라질로 떠난다고 한다. \n",
    "바람부는 허허벌판에서 5kg의 몸으로 버티는 펭귄들이 기특해 보였다. \n",
    "한 녀석은 길목에서 손님을 기다리고 있는 것 같더니 따라나서자 이렇게 줄행랑을 친다.\n",
    "따라 잡을 수 없을 만큼 빨랐다. \n",
    "다가서자 신기한 듯 이리저리 고개를 돌리며 구경이다.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잠시 마음을 놓는 순간 거세지는 게 이곳의 바람.  -> 잠시 마음을 놓는 순간 거세지는 게 이곳의 바람.\n",
      "마을을 떠나는 길에는 구름까지 잔뜩 몰려오더니 결국 비바람이 몰아치기 시작했다.  -> 마을을 떠나는 길에는 구름까지 잔뜩 몰려오더니 결국 아수라장이 몰아치기 시작했다.\n",
      "하필 오늘은 펭귄 서식지에 오기로 한 날. -> 하필 오늘은 노래에 오기로 한 날.\n",
      "풀들이 다 휘어버릴 만큼 거센 바람을 보니 여기 사는 펭귄들이 걱정됐다.  -> 풀들이 다 휘어버릴 만큼 바람을 보니 여기 사는 걱정됐다.\n",
      "여기에서 돌아갈 수도 없다.  -> 여기에서 돌아갈 수도 없다.\n",
      "나는 오늘 친구와 만났다.  -> 나는 오늘 친구와 만났다.\n",
      "나는 친구와 피자를 먹고 맥주를 마셨다.  -> 나는 친구와 피자를 먹고 맥주를 마셨다.\n",
      "나와 친구는 밥을 먹고 보드게임 카페를 갔다.  -> 나와 친구를 밥을 먹고 보드게임 카페를 갔다.\n",
      "거기서 할리갈리를 했는데 매우 재미 있었다. -> 거기서 할리갈리를 했는데 매우 재미 있었다.\n",
      "나 요즘 고양이 키우는데 너무 귀엽더라 ㅎㅎ -> 나 요즘 고양이 키우는데 너무 귀엽더라\n",
      "이 겜 진짜 핵노잼인데 -> 이 진짜 핵노잼인데\n",
      "이럴거면 게임 왜 만듦?? ㅋㅋㅋ ㅈㄴ 노어이 -> 이럴거면 난 좀?? ㅋㅋㅋㄴ ㅇ이 노어\n",
      "노잼이네요, 회사 문 닫으셈 ㅅㄱㅋ -> 노잼이네요, 회사 문 닫으셈 사살하라ㄱㅋ\n"
     ]
    }
   ],
   "source": [
    "texts = \"\"\"\n",
    "잠시 마음을 놓는 순간 거세지는 게 이곳의 바람. \n",
    "마을을 떠나는 길에는 구름까지 잔뜩 몰려오더니 결국 비바람이 몰아치기 시작했다. \n",
    "하필 오늘은 펭귄 서식지에 오기로 한 날.\n",
    "풀들이 다 휘어버릴 만큼 거센 바람을 보니 여기 사는 펭귄들이 걱정됐다. \n",
    "여기에서 돌아갈 수도 없다. \n",
    "나는 오늘 친구와 만났다. \n",
    "나는 친구와 피자를 먹고 맥주를 마셨다. \n",
    "나와 친구는 밥을 먹고 보드게임 카페를 갔다. \n",
    "거기서 할리갈리를 했는데 매우 재미 있었다.\n",
    "나 요즘 고양이 키우는데 너무 귀엽더라 ㅎㅎ\n",
    "이 겜 진짜 핵노잼인데\n",
    "이럴거면 게임 왜 만듦?? ㅋㅋㅋ ㅈㄴ 노어이\n",
    "노잼이네요, 회사 문 닫으셈 ㅅㄱㅋ\n",
    "\"\"\".strip().split(\"\\n\")\n",
    "import numpy as np\n",
    "\n",
    "def getRandomSamplesOnNSphere(N, R, numberOfSamples):\n",
    "    # Return 'numberOfSamples' samples of vectors of dimension N \n",
    "    # with an uniform distribution on the (N-1)-Sphere surface of radius R.\n",
    "    # RATIONALE: https://mathworld.wolfram.com/HyperspherePointPicking.html\n",
    "    \n",
    "    X = np.random.default_rng().normal(size=(numberOfSamples , N))\n",
    "    return R / np.sqrt(np.sum(X**2, 1, keepdims=True)) * X\n",
    "\n",
    "def paraphrase(text, r=5):\n",
    "    x = task.encode(text)\n",
    "    e = torch.tensor(getRandomSamplesOnNSphere(512, r, 1), dtype=torch.float)\n",
    "    # e = torch.rand_like(x) * 0.5\n",
    "    print(text, \"->\", task.generate(x + e, max_length=64, early_stopping=True)[0])\n",
    "\n",
    "for text in texts:\n",
    "    paraphrase(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
