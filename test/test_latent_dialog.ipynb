{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heegyukim\\Desktop\\project\\latent-diffusion-lm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from src import model_utils\n",
    "from src.dataset.latent_dialog import LatentDialogDataset\n",
    "from src.task.latent_dialog import LatentDialogTask\n",
    "from hydra import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'optimizer': {'cls': 'adam', 'learning_rate': 1e-05}, 'logger': {'name': 'wandb'}, 'dataset': {'train': ['heegyu/aihub_daily_conv_2022_CRF'], 'test': ['heegyu/aihub_daily_conv_2022_CRF']}, 'model': {'model': 'klue/roberta-small', 'autoencoder': 'checkpoint/optimus-v2-44M/optimus-v2-tiny-vae.ckpt', 'latent_dim': 512, 'max_seq_len': 512, 'free_bit': 2.0, 'use_plm': True, 'is_vae': True}, 'trainer': {'train_batch_size': 8, 'train_epochs': 20, 'shuffle': False, 'eval_batch_size': 8, 'eval_strategy': 'epoch', 'project': 'latent-dialog', 'run_name': 'roberta-small-opt44M', 'num_sanity_val_steps': 0, 'limit_train_batches': 110, 'limit_val_batches': 1}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if GlobalHydra.instance().is_initialized():\n",
    "    GlobalHydra.instance().clear()\n",
    "initialize(\"../config/\")\n",
    "config = compose(\"latent-dialog\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LatentDialogDataset(\n",
    "    [\"heegyu/aihub_daily_conv_2022_CRF\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 140329,\n",
       " 'depth': 0,\n",
       " 'context': '0 : **는 게임 좋아하니?\\n1 : 게임 어떤거 좋아해?',\n",
       " 'response': '2 : 게임 잘 못해서,'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDialogTask(\n",
       "  (model): BertVAE(\n",
       "    (encoder): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): RobertaPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (proj): Linear(in_features=768, out_features=1024, bias=False)\n",
       "  )\n",
       "  (autoencoder): OptimusTask(\n",
       "    (model): Optimus(\n",
       "      (encoder): RobertaModel(\n",
       "        (embeddings): RobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(32000, 512, padding_idx=1)\n",
       "          (position_embeddings): Embedding(66, 512, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 512)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): RobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pooler): RobertaPooler(\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "      )\n",
       "      (decoder): OptimusDecoder(\n",
       "        (transformer): OptimusGPT2(\n",
       "          (wte): Embedding(32000, 512)\n",
       "          (wpe): Embedding(65, 512)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (h): ModuleList(\n",
       "            (0): Block(\n",
       "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (c_attn): Conv1D()\n",
       "                (c_proj): Conv1D()\n",
       "                (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): GPT2MLP(\n",
       "                (c_fc): Conv1D()\n",
       "                (c_proj): Conv1D()\n",
       "                (act): NewGELUActivation()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): Block(\n",
       "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (c_attn): Conv1D()\n",
       "                (c_proj): Conv1D()\n",
       "                (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): GPT2MLP(\n",
       "                (c_fc): Conv1D()\n",
       "                (c_proj): Conv1D()\n",
       "                (act): NewGELUActivation()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear_mem): Linear(in_features=512, out_features=1024, bias=False)\n",
       "          (linear_emb): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (lm_head): Linear(in_features=512, out_features=32000, bias=False)\n",
       "      )\n",
       "      (proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.model.max_seq_len = 16\n",
    "task = LatentDialogTask(config)\n",
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': [140329, 140329, 140329, 140329, 140329], 'depth': [0, 0, 0, 0, 0], 'context': ['0 : **는 게임 좋아하니?', '0 : **는 게임 좋아하니?\\n1 : 게임 어떤거 좋아해?', '0 : **는 게임 좋아하니?\\n1 : 게임 어떤거 좋아해?\\n2 : 게임 잘 못해서,', '0 : **는 게임 좋아하니?\\n1 : 게임 어떤거 좋아해?\\n2 : 게임 잘 못해서,\\n0 : 우린 완전 게임 좋아해', '0 : **는 게임 좋아하니?\\n1 : 게임 어떤거 좋아해?\\n2 : 게임 잘 못해서,\\n0 : 우린 완전 게임 좋아해\\n1 : 나는 별로 안 좋아해'], 'response': ['1 : 게임 어떤거 좋아해?', '2 : 게임 잘 못해서,', '0 : 우린 완전 게임 좋아해', '1 : 나는 별로 안 좋아해', '2 : 어떤 게임 주로 하세요?']}\n",
      "16\n",
      "16\n",
      "['[CLS] 0 : * * 는 게임 좋아하니? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] * * 는 게임 좋아하니? 1 : 게임 어떤거 좋아해? [SEP]', '[CLS]? 1 : 게임 어떤거 좋아해? 2 : 게임 잘 못해서, [SEP]', '[CLS] 좋아해? 2 : 게임 잘 못해서, 0 : 우린 완전 게임 좋아해 [SEP]', '[CLS], 0 : 우린 완전 게임 좋아해 1 : 나는 별로 안 좋아해 [SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(39.3235, grad_fn=<MeanBackward0>),\n",
       " Normal(loc: torch.Size([5, 512]), scale: torch.Size([5, 512])))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.step(dataset[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def pad_to_multiple_of(pad_size: int, multiple_of: int):\n",
    "    if pad_size % multiple_of == 0:\n",
    "        return pad_size\n",
    "    else:\n",
    "        return multiple_of * ((pad_size // multiple_of) + 1)\n",
    "\n",
    "pad_to_multiple_of(18, 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
