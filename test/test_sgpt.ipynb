{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/latent-diffusion-lm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.task.sgpt import SGPTTask\n",
    "from coop.models.sgpt import SentenceGPT\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from hydra import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1076252/1463963186.py:3: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(\"../config/\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'optimizer': {'cls': 'adam', 'learning_rate': 0.001}, 'logger': {'name': 'wandb'}, 'dataset': {'train': ['heegyu/aihub_daily_conv_2022'], 'test': ['heegyu/aihub_daily_conv_2022']}, 'model': {'model': 'sgpt-base.json', 'autoencoder': 'checkpoint/optimus-v2-56M/optimus-v2-mini-vae.ckpt', 'latent_dim': 512, 'free_bit': 0.5, 'max_seq_len': 16, 'is_vae': False}, 'trainer': {'train_batch_size': 8, 'train_epochs': 10, 'shuffle': False, 'eval_batch_size': 8, 'eval_strategy': 'epoch', 'project': 'sgpt', 'run_name': 'base-opt56M', 'num_sanity_val_steps': 1, 'limit_train_batches': 1, 'limit_val_batches': 1}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if GlobalHydra.instance().is_initialized():\n",
    "    GlobalHydra.instance().clear()\n",
    "initialize(\"../config/\")\n",
    "config = compose(\"sgpt\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgpt = SGPTTask(config).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration heegyu--aihub_daily_conv_2022-ee6ad77711466ab5\n",
      "Reusing dataset parquet (/home/ubuntu/.cache/huggingface/datasets/heegyu___parquet/heegyu--aihub_daily_conv_2022-ee6ad77711466ab5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Using custom data configuration heegyu--aihub_daily_conv_2022-ee6ad77711466ab5\n",
      "Reusing dataset parquet (/home/ubuntu/.cache/huggingface/datasets/heegyu___parquet/heegyu--aihub_daily_conv_2022-ee6ad77711466ab5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 140329,\n",
       " 'depth': 18,\n",
       " 'speakers': [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],\n",
       " 'utterances': ['**는 게임 좋아하니?',\n",
       "  '게임 어떤거 좋아해?',\n",
       "  '게임 잘 못해서,',\n",
       "  '우린 완전 게임 좋아해',\n",
       "  '나는 별로 안 좋아해',\n",
       "  '어떤 게임 주로 하세요?',\n",
       "  '그럼... 보드 게임은 어대',\n",
       "  '게임은 너무 시간낭비야',\n",
       "  '보드게임은 단합하기 좋죵',\n",
       "  '우리는 음 맞고도 치고 루미 큐브도 하고',\n",
       "  '게임 할 시간에 난 책을 읽어',\n",
       "  '요즘 보드게임 카페 잘 되어 있잖아용',\n",
       "  '너 책 2년 동안 한권도 안 읽었...',\n",
       "  '책을 읽는 게 좋아',\n",
       "  '와우 엄청난 지식인',\n",
       "  '보드 게임 카페가 있어?',\n",
       "  '책은 정말 너무신기해',\n",
       "  '넵 요즘 애들 다 보드게임 카페가용 키키'],\n",
       " 'eous': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(sgpt.train_dataloader()))\n",
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, output = sgpt.step(batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(42.9597, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': Normal(loc: torch.Size([16, 512]), scale: torch.Size([16, 512])),\n",
       " 'attention_mask': tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'),\n",
       " 'labels': Normal(loc: torch.Size([16, 512]), scale: torch.Size([16, 512]))}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item, sents = sgpt._item_for_train(batch[0][\"utterances\"][:4])\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(324.8142, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " SentenceGPTOutput(latent=Normal(loc: torch.Size([8, 16, 512]), scale: torch.Size([8, 16, 512])), zkl=tensor(0., device='cuda:0', grad_fn=<DivBackward0>), zkl_real=tensor(0.2608, device='cuda:0', grad_fn=<DivBackward0>)))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgpt.step(batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6543,  0.4514, -0.4496, -0.2011,  0.1571, -0.0082, -0.0254,  0.3639,\n",
       "        -0.5781, -0.0607, -0.2672, -0.3125, -0.4056,  0.3304,  0.1396, -0.0600,\n",
       "         0.7875,  0.2400,  0.0881, -0.0476, -0.5376,  0.5642, -0.0779,  0.3077,\n",
       "         0.2767, -0.2557,  0.1148, -0.0906,  0.1007, -0.4733, -0.4064,  0.0548,\n",
       "        -0.3299, -0.5833, -0.4356, -0.6809,  0.7531, -0.3078,  0.0793,  0.7509,\n",
       "        -0.0198,  0.5229, -0.0683, -0.5755, -0.1218,  0.0679,  0.3972,  0.1757,\n",
       "         0.2055,  0.7688,  0.2889,  0.2690,  0.5317,  0.8551, -0.2805,  0.2565,\n",
       "        -0.0550, -0.2084,  0.2662,  0.6917,  0.1499, -0.1584,  0.2070, -0.0642,\n",
       "         0.0641,  0.3821,  0.5269,  0.1041,  0.1698, -0.0727,  0.2339, -0.1018,\n",
       "         0.1608,  0.0769, -0.2124,  0.0286,  0.5587,  0.1368, -0.4899, -0.4216,\n",
       "        -0.1526, -0.2055,  0.1048,  0.3757, -0.3938, -0.1050, -0.2472, -0.1659,\n",
       "        -0.5944, -0.0657, -0.1364,  0.2998, -0.3014,  0.4246,  0.1183, -0.1263,\n",
       "         0.1283,  0.1112, -0.2316,  0.7676,  0.5236,  0.5887, -0.4700,  0.1886,\n",
       "         0.3203,  0.3936,  0.1503, -0.2294,  0.1134,  0.7738, -0.3005, -0.0200,\n",
       "         0.3938,  0.3719,  0.0606, -0.1101,  0.2349,  0.0839, -0.5428, -0.2380,\n",
       "         0.4288, -0.4937, -0.0037, -0.5851,  0.6202,  0.2273, -0.2988,  0.2453,\n",
       "         0.3336,  0.0344,  0.3484, -0.4035, -0.7090,  0.3462, -0.1302,  0.3321,\n",
       "        -0.5079, -0.4017, -0.2748,  0.0086, -0.4434,  0.8570, -0.1315,  0.0207,\n",
       "        -0.0457, -0.1758, -0.0270, -0.6308, -0.3805,  0.0861, -0.1419,  0.8773,\n",
       "         0.0171, -0.1103, -0.1284,  0.1422,  0.0745,  0.1613,  0.1359, -0.1571,\n",
       "        -0.2512, -0.0566, -0.1802,  0.0515,  0.4022, -0.3171,  0.0016, -0.5959,\n",
       "        -0.5302, -0.1621, -0.0149,  0.2360,  0.1397,  0.3676, -0.1664,  0.1130,\n",
       "        -0.4028, -0.2540, -0.1038, -0.1925,  0.0818,  0.1386,  0.2785, -0.0593,\n",
       "        -0.1630,  0.3975, -0.0085,  0.2499, -0.9323, -0.5393,  0.4423, -0.1085,\n",
       "         0.5338,  0.4999,  0.2370,  0.4329,  0.6633,  0.3834,  0.0499,  0.6826,\n",
       "         0.4664, -0.2908,  0.0429,  0.8300,  0.8317, -0.6113,  0.0363,  0.0944,\n",
       "        -0.2102,  0.3103,  0.0332,  0.1564, -0.3005,  0.3437,  0.6186, -0.1035,\n",
       "         0.7185,  0.0119,  0.1677, -0.4090, -0.7394,  0.7600,  0.4660,  0.1484,\n",
       "        -0.2621, -0.1564, -0.0673, -0.5892, -0.0591,  0.1934, -0.3273, -0.0851,\n",
       "        -0.0866,  0.0909, -0.1139,  0.1328, -0.0885, -0.1967,  0.3529,  0.1367,\n",
       "        -0.8738,  0.2627, -0.4255,  0.0565, -0.1497, -0.0230, -0.1566,  0.5379,\n",
       "         0.1895,  0.1538,  0.0982, -0.2269, -0.3671, -0.0311,  0.2486, -0.3422,\n",
       "         0.3662, -0.9556,  0.7387,  0.4250,  0.3296,  0.0321,  0.2229, -0.7001,\n",
       "         0.0697,  0.3258, -0.0493,  0.0654,  0.0029,  0.2943, -0.5375,  0.3831,\n",
       "        -0.4487,  0.8486,  0.0116,  0.3495, -0.0115, -0.4244,  0.6240, -0.5290,\n",
       "         0.0872,  0.9562,  0.0743,  0.5660, -0.1942,  0.3725,  0.5983, -0.0862,\n",
       "         0.1326,  0.3436,  0.0998, -0.6599, -0.4742, -0.1495,  0.1977, -0.3702,\n",
       "        -0.0328,  0.2171,  0.3114, -0.3438,  0.3580, -0.5872, -0.3781,  0.1969,\n",
       "         0.4134,  0.2280,  0.4032, -0.1025,  0.4864,  0.1383, -0.4816,  0.3986,\n",
       "        -0.2032, -0.0400,  0.0298,  0.3283, -0.6437,  0.2990, -0.5458,  0.1947,\n",
       "         0.1440, -0.1152,  0.2677,  0.1155,  0.4776, -0.0659, -0.0215, -0.7919,\n",
       "        -1.0086,  0.1385,  0.3955,  0.3193, -0.0833,  0.2382, -0.0220, -0.1575,\n",
       "        -0.2786, -0.3096,  0.4222,  0.3349,  0.3363,  0.6185, -0.1521, -0.1947,\n",
       "         0.5286, -0.3585,  0.0137, -0.1374,  0.1737, -0.0549,  0.3877,  0.5352,\n",
       "        -0.5119,  1.1293, -0.2394, -0.0898,  0.5434,  0.2133, -0.1714, -0.0129,\n",
       "         0.0946,  0.1122,  0.0473, -0.2328,  0.1306, -0.2042, -0.1527,  0.2955,\n",
       "         0.0981,  0.0045,  0.0350,  0.5483, -0.5254, -0.1144,  0.6417,  0.8228,\n",
       "         0.1337, -0.0517,  0.3970,  0.3677, -0.5210, -0.2319, -0.2564, -0.0319,\n",
       "         0.2133,  0.2859, -0.0397,  0.1833, -0.6901, -0.0818, -0.0998,  0.5348,\n",
       "        -0.3255, -0.1208,  0.4230, -0.0333,  0.2610, -0.1089, -0.0685, -0.0623,\n",
       "        -0.1491, -0.1077, -0.1603,  0.8269, -0.1598,  0.3387, -0.4970,  0.0234,\n",
       "        -0.2355,  0.5364,  0.3700, -0.1151,  0.1368,  0.3851,  0.1812,  0.3989,\n",
       "         0.4268,  0.3452, -0.4432, -0.1564, -0.0872, -0.7430,  0.2135,  0.8433,\n",
       "         0.0321,  0.6152,  0.2752,  0.5470, -0.3111, -0.1281,  0.7913, -0.4736,\n",
       "         0.1940,  0.7799,  0.6182, -0.3372, -0.1078, -0.1749, -0.2230,  0.5485,\n",
       "        -0.0959, -0.2535, -0.0922, -0.2498, -0.1006,  0.0760, -0.3137,  0.2071,\n",
       "         0.0055, -0.3134,  0.7360, -0.1950,  0.1437, -0.2992,  0.0993,  0.1530,\n",
       "         0.2060,  0.0556,  0.7701,  0.0149,  0.5441,  0.0104, -0.7477, -0.2300,\n",
       "         0.3820,  0.0559, -0.1039,  0.5644, -0.4937,  0.3850,  0.2909, -0.0822,\n",
       "        -0.1357, -0.4286, -0.2675,  0.2063, -0.6757, -0.1786,  0.4328, -0.0084,\n",
       "        -0.8981, -0.3073, -0.1805,  0.3400,  0.0992, -0.0084, -0.2403, -0.0021,\n",
       "        -0.2258, -0.6564,  0.0231,  0.3519, -0.5697,  0.7481,  0.0970,  0.0024,\n",
       "        -0.4179,  0.8331, -0.0999, -0.1231, -0.1300,  0.1462, -0.3138,  0.6548,\n",
       "        -0.3939, -0.8241, -1.1730,  0.5021, -0.2653,  0.3333, -0.4279,  0.3787],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# item[\"inputs\"].loc[3] - item[\"labels\"].loc[2]\n",
    "item[\"labels\"].loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m hidden \u001b[38;5;241m=\u001b[39m sgpt\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m----> 2\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mrsample()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m      3\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m )\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m      5\u001b[0m x \u001b[38;5;241m=\u001b[39m sgpt\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mreparameterize(hidden)\n\u001b[1;32m      6\u001b[0m hidden\u001b[38;5;241m.\u001b[39mshape, x\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "hidden = sgpt.model.model(\n",
    "    inputs_embeds = item[\"inputs\"].rsample().unsqueeze(0),\n",
    "    attention_mask = item[\"attention_mask\"].unsqueeze(0)\n",
    ").last_hidden_state\n",
    "x = sgpt.model.reparameterize(hidden)\n",
    "hidden.shape, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal(loc: torch.Size([1, 16, 512]), scale: torch.Size([1, 16, 512]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.distributions import Normal, kl_divergence\n",
    "\n",
    "labels = Normal(\n",
    "    item[\"labels\"].loc.unsqueeze(0),\n",
    "    item[\"labels\"].scale.unsqueeze(0)\n",
    ")\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0166e-01,  2.6448e-01,  4.6458e-01,  1.0069e-01, -1.2405e-01,\n",
      "        -5.1134e-02, -1.4447e-01,  1.1437e-01,  5.2955e-01, -1.9023e-02,\n",
      "         2.8022e-01,  3.5747e-01,  1.6090e-01, -4.1626e-02,  7.3284e-02,\n",
      "        -1.9380e-01,  1.1013e+00,  9.2238e-02,  8.4387e-01,  1.7632e-01,\n",
      "        -2.2174e-01, -4.0417e-01,  7.6656e-02,  3.6906e-01,  9.0171e-02,\n",
      "        -1.3099e-01,  1.6960e-01, -5.4958e-01,  3.4345e-01,  1.7964e-01,\n",
      "         5.0326e-01,  4.7890e-01, -2.0597e-02,  1.7637e-01, -2.6835e-01,\n",
      "         1.7770e-01, -7.9549e-01, -1.0125e-01, -1.0733e-02,  9.0773e-02,\n",
      "         1.6803e-01,  6.4719e-01,  3.9788e-01,  7.4002e-01,  6.0794e-01,\n",
      "         3.9314e-01,  4.8075e-01, -4.4489e-01, -4.4656e-01,  5.4711e-01,\n",
      "        -3.4176e-01, -7.0742e-02, -3.0671e-01, -2.6214e-01,  1.4209e-01,\n",
      "        -2.7931e-01,  3.9125e-01, -4.0892e-01, -7.2583e-01, -3.9219e-01,\n",
      "         3.6348e-02,  1.3070e-01,  2.7684e-01,  3.2558e-01, -2.7234e-01,\n",
      "        -1.1607e-01,  2.2687e-04, -2.6895e-01, -3.6244e-01, -6.1764e-02,\n",
      "         3.7537e-02, -1.1661e-01,  6.1560e-01,  5.2251e-01,  3.6071e-01,\n",
      "        -4.1198e-02,  3.0754e-01, -1.4164e-01,  4.1526e-01, -1.8348e-01,\n",
      "        -7.3576e-01, -1.4624e-01, -2.7801e-01, -1.5189e-01,  4.6064e-01,\n",
      "         3.4223e-01,  1.6201e-01, -4.5055e-01,  5.2783e-01,  2.1306e-01,\n",
      "         3.9551e-01,  1.6018e-01,  8.3903e-02,  3.9555e-01,  4.0223e-01,\n",
      "         8.1741e-02,  7.2691e-02, -1.8705e-01, -3.4472e-01,  1.5177e-02,\n",
      "        -4.4923e-01, -2.7110e-01, -6.0692e-01,  5.3354e-02,  1.5940e-01,\n",
      "         4.7395e-01, -7.4437e-02, -5.4081e-01, -1.0383e-01, -1.9053e-01,\n",
      "        -3.3134e-01, -2.2933e-01,  5.3571e-01,  7.3554e-01, -4.9490e-01,\n",
      "        -4.6313e-02,  7.2159e-01, -3.0795e-01,  1.2015e-01,  4.6069e-02,\n",
      "        -5.4716e-01,  2.9642e-01, -2.8675e-01,  1.6236e-02, -2.6475e-01,\n",
      "        -5.0221e-02, -2.7789e-01,  3.6693e-01,  1.3412e-01,  4.2059e-03,\n",
      "         9.0965e-02, -2.6017e-01, -6.0571e-02,  7.3716e-02,  2.3248e-02,\n",
      "         7.5395e-02,  3.7207e-01,  3.5106e-01, -6.1289e-01, -1.3330e-01,\n",
      "         8.2622e-02,  2.9637e-01,  1.5038e-01,  3.8068e-02,  2.1629e-01,\n",
      "        -2.7436e-01, -2.5386e-01, -3.5808e-01, -1.1197e+00,  1.5006e-01,\n",
      "         2.3750e-01,  3.8912e-01, -6.8386e-02, -6.5915e-01, -2.8284e-02,\n",
      "        -8.2691e-01,  4.0371e-02, -5.6913e-01, -2.2354e-01,  5.2303e-01,\n",
      "        -3.3442e-02, -1.7967e-01, -2.7593e-01,  3.5302e-01,  5.8330e-01,\n",
      "        -2.0036e-01, -2.8930e-01,  5.9976e-02, -2.3129e-01,  5.3076e-01,\n",
      "        -2.3161e-01,  2.2861e-01,  3.2168e-01, -4.3608e-01,  3.6056e-02,\n",
      "         4.2103e-01, -2.9874e-01,  3.6076e-01,  8.1117e-02, -1.2975e-01,\n",
      "        -2.8699e-01, -1.1492e-01, -6.5138e-01, -3.3404e-01,  2.2990e-01,\n",
      "        -5.3618e-01, -2.1210e-01, -3.3088e-01,  1.7906e-02,  5.4934e-01,\n",
      "         2.2054e-01, -6.0841e-01, -6.9526e-03,  3.3248e-01,  6.8723e-03,\n",
      "         2.2241e-01, -2.9783e-01, -7.5067e-01,  4.5526e-01, -1.9323e-01,\n",
      "        -5.8464e-02,  2.8513e-01, -2.5609e-01, -7.4622e-02, -3.7621e-01,\n",
      "        -2.7514e-01,  7.8913e-02,  1.8499e-01,  2.7617e-01, -1.6885e-01,\n",
      "         1.7125e-01, -2.9954e-01, -5.7721e-01,  6.0173e-02, -1.4453e-01,\n",
      "         5.9530e-01, -6.0693e-01,  5.6218e-01, -5.6210e-01, -5.9293e-02,\n",
      "         1.9160e-01, -8.3824e-01, -4.5230e-01, -3.6127e-01,  2.6998e-01,\n",
      "        -5.5006e-01, -2.0634e-01, -1.4360e-01,  1.9234e-02,  2.2073e-02,\n",
      "        -3.1253e-01,  1.8183e-01, -1.9572e-01, -1.5695e-01,  1.5988e-02,\n",
      "         4.2149e-01, -7.0676e-01, -7.6097e-01,  5.0228e-01,  5.7912e-02,\n",
      "         1.3576e-01,  7.3253e-02, -5.5494e-01,  3.7011e-01, -1.6185e-01,\n",
      "        -4.6946e-01, -6.2545e-01, -4.9450e-02,  9.8939e-01,  1.0873e-01,\n",
      "        -2.5797e-01,  2.4423e-01, -4.8729e-02, -1.0961e-02, -9.5689e-02,\n",
      "        -2.5774e-01,  6.4918e-02, -7.7223e-02, -2.2893e-01, -1.9216e-01,\n",
      "         1.8599e-01,  3.6994e-01,  4.6237e-02,  4.9634e-01, -1.3000e-01,\n",
      "        -3.2309e-01,  3.4855e-01, -4.2707e-01,  2.6523e-02, -3.0895e-01,\n",
      "        -4.1076e-01, -3.6196e-01,  2.8411e-01, -6.5514e-02,  2.9661e-01,\n",
      "        -5.3147e-01,  3.1170e-01,  6.5914e-01,  4.0405e-01, -5.1264e-01,\n",
      "         6.2857e-02, -1.7284e-01,  8.4184e-01,  2.1049e-01, -5.2210e-01,\n",
      "         3.1595e-01, -3.6681e-02,  3.5338e-01, -1.7072e-01,  3.4897e-02,\n",
      "        -2.1846e-01,  5.6417e-01,  2.8268e-01, -6.9117e-02,  5.0127e-01,\n",
      "        -6.1327e-01, -2.8064e-01, -1.5187e-01, -1.1815e-01, -1.8324e-01,\n",
      "        -5.0160e-02,  4.4803e-02,  9.8757e-02,  2.8890e-01,  1.3924e-01,\n",
      "        -9.4842e-02, -2.2525e-01,  5.0518e-01,  2.5869e-01,  6.1433e-01,\n",
      "         1.8569e-01, -3.2148e-01, -4.8556e-01, -2.2274e-01, -4.3261e-01,\n",
      "         2.6900e-01,  1.4272e-01, -5.9336e-02,  7.0397e-02,  1.7427e-01,\n",
      "         7.1034e-01, -2.0042e-01,  5.8759e-01,  6.8344e-02, -2.2413e-01,\n",
      "         3.2460e-01,  4.3674e-01, -2.2553e-01, -2.3609e-01, -4.0683e-01,\n",
      "         3.3984e-01,  1.5630e-01,  2.8995e-01, -1.2948e-01, -7.0831e-01,\n",
      "         5.9399e-01,  1.4602e-01,  3.3806e-01, -1.9211e-01,  1.4535e-01,\n",
      "         3.9888e-01,  2.1713e-01,  2.5934e-01, -4.2541e-02, -8.7617e-01,\n",
      "        -2.9714e-01, -3.0809e-01, -6.3623e-01,  4.8156e-01, -3.2286e-01,\n",
      "        -3.7964e-02,  2.1735e-01,  4.2631e-01,  5.2317e-01, -6.5835e-01,\n",
      "         1.5155e-02,  1.8193e-01,  1.7193e-01,  4.4541e-02, -5.8563e-01,\n",
      "         6.9942e-02,  1.7092e-01, -5.6155e-01,  3.0635e-01, -9.8620e-03,\n",
      "         1.0578e-01, -9.3809e-02,  5.7440e-01, -1.5635e-01,  1.6229e-01,\n",
      "         4.5916e-01,  9.4176e-02, -6.7046e-01,  3.1491e-01,  4.4583e-01,\n",
      "        -8.0306e-01,  1.1808e-01,  1.0641e-01, -1.9740e-01,  6.7404e-02,\n",
      "         3.2159e-01,  2.7101e-01, -1.2578e-01,  5.1103e-01, -1.7545e-01,\n",
      "         8.9525e-01, -6.4643e-01, -4.2853e-01, -1.3646e-01,  7.7879e-02,\n",
      "         1.3411e-01,  4.1613e-01, -7.8858e-01,  3.6745e-02, -8.4204e-03,\n",
      "         8.0198e-02,  1.4876e-02,  1.0820e-01,  3.3174e-01,  4.0886e-01,\n",
      "         8.7196e-01, -7.8535e-02,  2.4204e-01,  2.9743e-01, -8.6555e-01,\n",
      "         2.9365e-01, -9.9672e-03,  3.4741e-02, -1.4608e-01, -8.1257e-01,\n",
      "         4.3998e-01,  4.0535e-01,  4.3317e-01, -2.4574e-01, -2.9825e-01,\n",
      "         6.2417e-01, -2.6741e-01, -4.2314e-02,  3.3152e-02, -2.4543e-01,\n",
      "         6.3680e-01,  6.2761e-01, -2.5183e-01,  4.5455e-01,  4.2125e-01,\n",
      "         2.0972e-01,  4.5685e-01, -4.5635e-01,  1.6889e-01,  1.1491e-01,\n",
      "        -2.2291e-02,  2.1095e-02, -7.4251e-02, -9.0196e-02,  6.2297e-01,\n",
      "         3.3649e-01, -3.1688e-02,  3.8870e-01, -3.3094e-01,  1.8062e-01,\n",
      "        -4.2337e-01,  3.3824e-01, -3.5491e-01,  5.0927e-01,  5.6322e-01,\n",
      "         2.8225e-02,  7.2463e-02,  4.4793e-01, -5.8691e-01, -2.7691e-02,\n",
      "         8.0589e-01,  1.5816e-01,  1.3195e-01,  3.0109e-02,  7.2165e-02,\n",
      "        -1.1245e-01,  5.8278e-01,  3.9749e-02,  3.6279e-01,  6.2839e-01,\n",
      "         1.0262e-01, -1.9061e-01,  1.1738e-02,  1.7280e-01,  4.2340e-01,\n",
      "        -5.5208e-02, -9.1506e-01,  1.0025e+00, -3.8197e-01, -2.3269e-01,\n",
      "         5.8361e-02,  6.2680e-01,  3.2440e-01, -2.1381e-01,  7.4027e-01,\n",
      "         1.2751e-01, -2.4027e-01, -1.0814e-01, -2.6318e-01,  8.7558e-02,\n",
      "        -2.6277e-01, -2.6949e-01, -1.1915e-01,  7.4802e-01, -2.5161e-01,\n",
      "         1.3492e-01,  3.0204e-01,  4.0225e-01,  4.3357e-02, -2.5283e-02,\n",
      "        -2.0326e-01, -2.5037e-01,  2.0941e-01,  1.8616e-02,  1.6610e-01,\n",
      "         3.4838e-01, -1.1664e-01,  1.5880e-01, -1.3966e-01,  1.2907e-01,\n",
      "         2.1776e-01, -2.5573e-01, -2.8968e-01, -9.2601e-02, -3.1237e-02,\n",
      "        -3.0190e-01, -1.2639e-01,  9.6939e-02,  8.0843e-02,  1.1105e-01,\n",
      "        -7.5512e-03, -3.7326e-01], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(labels.loc[0, 2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.2743e+01, 4.5951e+01, 5.6947e+01, 7.6523e+11, 7.5514e+11, 7.4267e+11,\n",
       "         7.5617e+11, 7.6696e+11, 7.6777e+11, 7.3560e+11, 7.6166e+11, 7.3166e+11,\n",
       "         7.4869e+11, 7.6578e+11, 7.6710e+11, 7.5890e+11]], device='cuda:0',\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def kldiv_loss(free_bit, p: Normal, q: Normal, attention_mask: torch.Tensor = None, use_free_bit: bool = True):\n",
    "    zkl_real = kl_divergence(p, q).mean(-1)\n",
    "\n",
    "    if attention_mask is not None:\n",
    "        attn_sum = attention_mask.sum()\n",
    "        zkl_real = zkl_real.masked_fill(attention_mask == 0, 0.0)\n",
    "    else:\n",
    "        attn_sum = p.loc.shape[0] * p.loc.shape[1]\n",
    "\n",
    "    if use_free_bit:\n",
    "        kl_mask = torch.gt(zkl_real, free_bit)\n",
    "        zkl = zkl_real[kl_mask].sum() / attn_sum\n",
    "        zkl_real = zkl_real.sum() / attn_sum\n",
    "\n",
    "        return zkl, zkl_real\n",
    "    else:\n",
    "        zkl_real = zkl_real.sum() / attn_sum\n",
    "        return zkl_real\n",
    "        \n",
    "# kldiv_loss(\n",
    "#     0.5,\n",
    "#     x,\n",
    "#     labels,\n",
    "#     attention_mask = item[\"attention_mask\"],\n",
    "#     use_free_bit=False\n",
    "# )\n",
    "kl_divergence(x, labels).mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimusTask(\n",
       "  (model): Optimus(\n",
       "    (encoder): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(32000, 512, padding_idx=1)\n",
       "        (position_embeddings): Embedding(66, 512, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 512)\n",
       "        (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): RobertaPooler(\n",
       "        (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (decoder): OptimusDecoder(\n",
       "      (transformer): OptimusGPT2(\n",
       "        (wte): Embedding(32000, 512)\n",
       "        (wpe): Embedding(65, 512)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0): Block(\n",
       "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Block(\n",
       "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): Block(\n",
       "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): Block(\n",
       "            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (c_attn): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear_mem): Linear(in_features=512, out_features=2048, bias=False)\n",
       "        (linear_emb): Linear(in_features=512, out_features=512, bias=False)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=512, out_features=32000, bias=False)\n",
       "    )\n",
       "    (proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgpt.autoencoder.decoder(\n",
    "    **tgt,\n",
    "    past_key_values=(latent,),\n",
    "    latent_as_gpt_memory=True,\n",
    "    latent_as_gpt_emb=True,\n",
    "    return_dict=True,\n",
    "    **decoder_kwargs,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
